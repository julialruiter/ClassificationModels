---
title: "Assignment 9"
author: "Julia Ruiter"
date: "11/17/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Assignment 9 / Makeup Assignment

In the diabetes.csv dataset, the column label represents the class label of the
record (268 of 768 are 1 â€“ diabetic, the others are 0). You have to apply
three different classification techniques to predict the label of each record
(You may use SVM, Random Forests and Neural Networks).

```{r libraries}
library(tidyverse)
library(randomForest)  # random forests
#library(caret)         # confusion matrixes -- ran into errors so I found an alternative
library(InformationValue)   # confusion matrixes
library(e1071)         # svm
library(neuralnet)     # neural networks
```

## Exercise 1.

Split the dataset (with stratification) into training and testing (75/25).

```{r}
# import csv file
diabetes_df  <- read_csv("diabetes.csv")  # 768 tuples

# create test and training sets
set.seed(42)  # answer to life, the universe, and everything (as always)

shuffle_index <- sample(nrow(diabetes_df))  # shuffle lines -- index ordering
diabetes_df <- diabetes_df[shuffle_index, ]    # shuffle lines -- rearrange rows by new index order
head(diabetes_df)

split_index <- floor(0.75*nrow(diabetes_df))    # include 75% in training
diabetes_training <- diabetes_df[0:split_index, ]     # 75% = 576 tuples
diabetes_test <- diabetes_df[(split_index+1):(nrow(diabetes_df)), ]   # the rest

summary(diabetes_training)
summary(diabetes_test)
```

Exercises 2 through 4 will be repeated using 3 different methods.  

## METHOD 1:  Random Forests

### Exercise 2 - Random Forests

Train the model using the training data.

```{r}
rf_model <- randomForest(Outcome ~ ., data = diabetes_training, ntree = 4000, mtry = 3, importance = TRUE)  # desired output:  Outcome
rf_model
#  Mean of Squared residuals is 0.169 ==> data set is around 83% accurate with ntree = 1000, mtry = 5
#  Mean of Squared residuals is 0.166 ==> data set is around 83% accurate with ntree = 2000, mtry = 3 (better, but barely) ==> we'll use this one because it's fast enough and accurate enough
```

### Exercise 3 - Random Forests

Use the model to predict the labels of the records in the test data.

```{r}
pred_tree <- predict(rf_model, diabetes_test, type = "class")

#table(pred_tree, diabetes_test$Outcome)   # Checking classification accuracy

mean(pred_tree)
mean(diabetes_test$Outcome)

# these means are  quite close to each other (0.0004 0ff).  This means that the model is quite acceptable -- scores are near what they should be

```

### Exercise 4 - Random Forests

Compute and display the confusion matrix.

```{r}
# I will use 0.83 as the cutoff value for what "counts" as positive since the MSE was around 0.17 for the training model
confusionMatrix(diabetes_test$Outcome, pred_tree, 0.83)

# using the MSE for the cutoff threshold yielded fabulous results!
```



## METHOD 2:  SVM

### Exercise 2 - SVM

Train the model using the training data.

```{r}
svm_model <- svm(Outcome ~., data =  diabetes_training)
svm_model
# 415 support vectors?!? that's a lot for a 2 category system
```

### Exercise 3 - SVM

Use the model to predict the labels of the records in the test data.

```{r}
pred_svm <- predict(svm_model, diabetes_test, type = "class")

#table(pred_svm, diabetes_test$Outcome)   # Checking classification accuracy

mean(pred_svm)
mean(diabetes_test$Outcome)

# these means are somewhat close to each other (0.04 0ff).  This means that the model is acceptable -- scores are reasonably near what they should be
```

### Exercise 4 - SVM

Compute and display the confusion matrix.

```{r}
#I will use 0.865 as the cutoff value for what "counts" as positive since the gamma is 0.125
#confusionMatrix(diabetes_test$Outcome, pred_svm, 0.865)

# using the gamma for the cutoff threshold yielded barely decent results -- but too many false negatives--disastrous for health sciences.

# trying again with a more neutral bias:
confusionMatrix(diabetes_test$Outcome, pred_svm, 0.51)
# still yields an alarming number of false negatives, plus too many false positives.
# ==> SVM is not a good method for this dataset
```



## METHOD 3:  Neural Networks

### Exercise 2 - Neural Networks

Train the model using the training data.

```{r}
nn_model <- neuralnet(Outcome ~., data =  diabetes_training, hidden=c(3,1), linear.output=FALSE, threshold=0.01)
nn_model$result.matrix
plot(nn_model)  # this is so cool!  it visualises how the levels interact

#  note:  error percents change quite drastically even between runs with the same parameters
# hidden=c(2,1) ==> 60.764 error
# hidden=c(3,1) ==> 46.47 error ==> chosen model ( though my initial test yielded error of 56.412)
# hidden=c(2,2,1) ==> 65.58 error
```



### Exercise 3 - Neural Networks

Use the model to predict the labels of the records in the test data.

```{r}
pred_nn <- predict(nn_model, diabetes_test, type = "class")

mean(pred_nn)
mean(diabetes_test$Outcome)

# these means suprisingly a lot closer than I expected (given the awful "Error" in the earlier plotted model).  They only differ by around 0.001, with the true vlues being ever so slightly higher, meaning I think it will likely yield on the side of false positive?
```

### Exercise 4 - Neural Networks

Compute and display the confusion matrix.

```{r}
#I will use a neutral cutoff to star with:
confusionMatrix(diabetes_test$Outcome, pred_nn, 0.51)

# OOOH NOOO , THAT'S BAD.
# As seen in the nn model plot, error rate is around 50%
# ==> Neural Networks do not seem to be a good idea for this type of data
```
